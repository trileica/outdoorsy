{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - model design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outdoorsy is the most trusted RV rental and outdoor experiences marketplace on the planet. We have grown from a lofty white-board idea in 2015, to over $525M in GMV and offices worldwide in the US, Canada, Australia, Europe and the UK. We’ve mobilized the 56+ million idle RVs and camper vans around the world to ensure everyone has the access, choice, and opportunity to safely spend more time outside.\n",
    "\n",
    "Over 856,000 vacation nights were booked on Outdoorsy in 2019. 93% of reviewed Outdoorsy\n",
    "bookings receive 5-star ratings. With over 600,000 families and adventure seekers—and 50,000\n",
    "RVs, travel trailers, and campervans to choose from—that’s a lot of sunset selfies taken!\n",
    "\n",
    "As a RV rental company, we want to build a website for our potential customers and customers to view our listings, plans and RV models. As usual, there is a tracking mechanism behind the website to track user’s activities. \n",
    "\n",
    "As a data scientist, you will own the data work from data collection, processing and serve the analytical needs. Assuming you are revamping the search ranking algorithm for the website, you plan to build a model to get an idea of which listing should be ranked higher due to the good attributes associated with the listing. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How do you define the training set? What is the label?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have all needed activities collected from the websites for a certain period of time. For a given listing, we would like to know the attributes of the listing itself, such as price, title, description, color and so on. Also, we would like to aggregate the engememant data such as clicking, skiping, liking and so on for additional information.\n",
    "\n",
    "For label, we would like to get the aggregated number of times a listing is purchased/reserved and engaged in order to calculate a label for testing different models. For example, we can define $relavancy\\_score$ as below to train a Learning-To-Rank (LTR) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "relevancy\\_score =: \\frac{\\#{purchased}}  {\\#{clicked}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What would be the top 10 features you would consider for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \\# of purchase within $n$ months/weeks\n",
    "2. \\# of click within $n$ months/weeks\n",
    "3. \\# of skip within $n$ months/weeks\n",
    "10. sentimenmt of reviews\n",
    "7. customer ratings\n",
    "4. embedding of listing name\n",
    "5. embedding of listing description\n",
    "6. embedding of listing type\n",
    "7. embedding of listing location\n",
    "8. listing price\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What would be the top 3 ML models you would consider?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. LTR model (point-/pair-/list-wise)\n",
    "2. XGBoost for ranking\n",
    "3. Bayesian Personalized Ranking\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What would be the key model measurement to evaluate the model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NDCG@$k$ would be the key metric for evaluate the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What would be the KPI when doing A/B testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For success metric of an A/B testing, we will look at,\n",
    "- conversion rate\n",
    "- engagement rate (click-through rate)\n",
    "- revenue per visit\n",
    "- average order value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7a. How do you address the scalability, and performance issue in your process? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If scalability is a concern, there are a few possible approaches to tackle. Firstly, if the model itself is too big to fit, then we can downsize by extracting the appropriate subsets of data for the model. Additionally, the code itself can be optimized as well, such as using parallel or distributed computing, Furthermore, choosing a more scalable algorithm could be an option, too. If the scalability is on the serving side, then the infrastructure can be adequately optimized as well. Assuming the cost is not the major concern, then establishing a more scalable infrasture might be an option. Alternatively, using cloud service such as Google Cloud Platform or Amazon Web Service to spin up virtual machine and let them to scale it up and down can be considered, too. \n",
    "\n",
    "If performance is an issue, we could look at the data first, and see if the data is enough to make any reliable model, such as by comparing with the baseline model using pure popularity to rank. If not, then we would need to collect more meaningful data points, generated more features, or even change to using a better algorithm. It is unlikely, but not impossible, the definition of performance could be wrong sometimes. Before drawing the conclusion of performance is low, we should re-visit every detail in the whole process first, and let the data speak / decide if the performance is really lower than expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7b. How do you handle missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the percentage of missing, if it is not a serious missing such as less than 5%, we could use the most popular, average or median for numerical features, and using the most popular or creating another dummy variable for categorical features; randomly drawing a value from the population, or even do nothing. If the missing rate is somewhat serious, then the aforementioned approaches are still valid. In addition, boostrapping the samples with different imputations or using kNN to impute missing values can be considered as well. Some imputation methods using deep learning are available as well, but it might be slower than other approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
